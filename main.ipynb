{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b075c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70ebecf",
   "metadata": {},
   "source": [
    "# Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4cca51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(points, classes): # Creates a dataset of spiral shaped clusters\n",
    "    X = np.zeros((points * classes, 2))\n",
    "    y = np.zeros(points * classes, dtype='uint8')\n",
    "\n",
    "    for class_number in range(classes):\n",
    "        ix = range(points * class_number,\n",
    "                   points * (class_number + 1))\n",
    "        r = np.linspace(0.0, 1, points)  # radius\n",
    "        t = np.linspace(class_number * 4,\n",
    "                        (class_number + 1) * 4,\n",
    "                        points) + np.random.randn(points) * 0.2\n",
    "        X[ix] = np.c_[r * np.sin(t * 2.5),\n",
    "                      r * np.cos(t * 2.5)]\n",
    "        y[ix] = class_number\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X,Y = create_data(100,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0a3f3f",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16e0c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron_Layer():\n",
    "    def __init__(self, n_inputs,n_neurons):\n",
    "        self.weights = 0.1*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "        pass\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs,self.weights)+self.biases\n",
    "        pass\n",
    "    \n",
    "    def backward(self,dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d119b4",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94309a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU():\n",
    "    def forward(self,inputs):\n",
    "        self.output = np.maximum(0,inputs)\n",
    "        self.inputs = inputs\n",
    "        \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65454d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax():\n",
    "    def forward(self,inputs):\n",
    "        exps = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
    "        normalised = exps / np.sum(exps,axis=1,keepdims=True)\n",
    "        self.output = normalised\n",
    "    \n",
    "    def backward(self,dvalues): \n",
    "        # TODO Reread 216-225\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output,single_dvalues) in enumerate(zip(self.output,dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "                              \n",
    "            # Calculate sample-wise gradient.\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e138bc8",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98c7942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    def __init__(self,learning_rate=1.0):\n",
    "        self.learning_rate=learning_rate\n",
    "    \n",
    "    def update_params(self,layer : Neuron_Layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9eef51",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "055ecd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self,output,y):\n",
    "        sample_losses = self.forward(output,y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d728cd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self,y_pred,y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred,1e-7,1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape)==1: # Labels are class indicies\n",
    "            correct_confidences = y_pred_clipped[range(samples),y_true]\n",
    "        elif len(y_true.shape)==2: # One hot encoding\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true,axis=1)\n",
    "        \n",
    "        loss = -np.log(correct_confidences)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        if len(y_true.shape)==1:\n",
    "            # If it is sparse, convert to one-hot\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        self.dinputs = -y_true/dvalues # Calculate gradient\n",
    "        self.dinputs = self.dinputs/samples # Normalize gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc8e19",
   "metadata": {},
   "source": [
    "Below is a combined implementation of the softmax activation function and the Categorical Crossentropy loss function, which is more optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe3ceb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    def forward(self,inputs,y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        return self.loss.calculate(self.output,y_true)\n",
    "    \n",
    "    def backward(self,dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        if len(y_true.shape)==2: # If one hot, turn it into sparse (discrete)\n",
    "            y_true = np.argmax(y_true,axis=1)\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples),y_true] -=1\n",
    "        self.dinputs = self.dinputs / samples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00231ba7",
   "metadata": {},
   "source": [
    "# Spiral dataset\n",
    "In the below cells I will attempt to create and train a neural network on the spiral dataset created in cell 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e437c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the structure of the network\n",
    "\n",
    "\n",
    "layer1 = Neuron_Layer(2,64)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "layer2 = Neuron_Layer(64,3) # Output layer\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_SGD(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81d6a9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.096954255394673\n",
      "acc: 0.36\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "\n",
    "layer2.forward(activation1.output)\n",
    "loss = loss_activation.forward(layer2.output, Y)\n",
    "\n",
    "predictions = np.argmax(loss_activation.output,axis=1)\n",
    "y = Y.copy()\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(Y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "print('loss:', loss)\n",
    "print('acc:', accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6d6f39",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Backward Pass: Softmax + Categorical Crossentropy\n",
    "\n",
    "The backward pass through the **softmax** and **categorical crossentropy loss** can be implemented in two different ways.\n",
    "\n",
    "---\n",
    "\n",
    "### **Combined Implementation (Recommended)**\n",
    "\n",
    "This approach fuses the softmax activation and categorical crossentropy loss into a single backward pass.\n",
    "It is **computationally more efficient** and **numerically stable**.\n",
    "\n",
    "```python\n",
    "softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "softmax_loss.backward(activation2.output, Y)\n",
    "dvalues1 = softmax_loss.dinputs\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Separate Implementation (Less Optimal)**\n",
    "\n",
    "This approach computes the backward pass for the loss and activation **independently**, resulting in extra computation.\n",
    "\n",
    "```python\n",
    "activation = Activation_Softmax()\n",
    "activation.output = activation2.output\n",
    "\n",
    "loss = Loss_CategoricalCrossentropy()\n",
    "loss.backward(activation2.output, Y)\n",
    "\n",
    "activation.backward(loss.dinputs)\n",
    "dvalues2 = activation.dinputs\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb79f4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.60353485e-04  8.21591718e-04  1.64856392e-03  1.56066659e-04\n",
      "   3.01920954e-04 -4.11998028e-04 -1.18409140e-03  5.74029975e-04\n",
      "  -2.35125433e-06 -4.39047351e-03 -5.54947268e-04  5.71098071e-04\n",
      "   6.06602842e-04  1.01263125e-03  1.43516850e-03  2.61695293e-03\n",
      "   1.07257656e-03  2.91578689e-04  7.49834200e-04 -1.67464087e-04\n",
      "  -5.91174645e-04  1.50413433e-03  1.24583758e-04  4.37889165e-04\n",
      "  -8.22745084e-05 -3.29320089e-04  2.80925560e-04  4.27853738e-05\n",
      "   3.60705206e-04  5.78682263e-04 -4.86213290e-04 -1.74072340e-04\n",
      "   1.85531839e-05 -2.68101935e-03  3.70008477e-03 -2.27912662e-04\n",
      "   5.71246726e-05 -1.50773870e-03  1.23636026e-03 -9.59831504e-04\n",
      "   3.64413246e-04  1.47144837e-04  1.26062018e-03 -8.25836713e-04\n",
      "  -1.59421414e-03  1.39595074e-03 -2.86759415e-04 -1.62456500e-03\n",
      "   8.16717256e-05 -1.00062355e-04  1.45611067e-03  7.60009919e-04\n",
      "   1.64431820e-04 -2.44547569e-03 -1.11016325e-04  2.31122372e-03\n",
      "  -2.49965821e-04 -1.18413571e-03 -3.29019918e-04 -4.84269614e-04\n",
      "  -8.53349214e-04  2.67378033e-03  3.59789457e-05 -1.14174524e-03]\n",
      " [-1.81014181e-03 -1.30752636e-03 -1.73610723e-03 -1.44226921e-03\n",
      "   1.08181973e-03 -1.35102911e-03  1.85318061e-03  2.44349958e-03\n",
      "   2.30123991e-05  3.38219739e-03 -2.13445565e-04 -1.40463625e-04\n",
      "  -5.48824148e-03  3.20957027e-03 -1.40250085e-03 -1.38449145e-03\n",
      "   3.83413681e-03 -6.18795930e-05  9.64931294e-05 -5.72559409e-03\n",
      "  -1.42998596e-03 -2.09795236e-03  3.14467804e-03  1.11404902e-03\n",
      "   9.01954308e-04 -2.44139980e-04  2.04242542e-04  2.98568232e-04\n",
      "   1.81437824e-03  1.23540220e-05 -1.91271720e-03  1.61469207e-03\n",
      "   1.47258342e-03  4.35457640e-03 -2.96558018e-04  5.65520341e-04\n",
      "  -1.16728171e-04 -1.41981151e-03 -2.14491984e-03 -1.99731880e-03\n",
      "   2.56154245e-03 -1.61708854e-03 -3.03674904e-03  2.58863503e-03\n",
      "   2.51638397e-03 -1.06645887e-03 -6.46980847e-04 -1.43564306e-03\n",
      "  -8.41125818e-04  6.62341332e-04 -1.95686609e-03  2.04911615e-03\n",
      "   2.94484904e-05 -9.19208278e-04 -2.25096516e-04 -4.72313804e-04\n",
      "  -9.07685011e-04  1.65311112e-03 -2.44034692e-03 -7.99213842e-04\n",
      "  -2.29078242e-04 -5.32360497e-04 -1.06972356e-04 -2.66929293e-03]]\n",
      "[[-2.15275559e-03  3.79499807e-03 -1.82434933e-03  1.52585992e-04\n",
      "  -1.24238917e-03  2.55159471e-03  2.47794867e-03 -2.47512855e-03\n",
      "  -1.59782199e-04  4.44706784e-03  7.71814499e-04  1.48164351e-03\n",
      "  -5.96582340e-03 -6.21911729e-03 -1.76474313e-03 -2.24772776e-03\n",
      "  -6.97821226e-03 -5.84295360e-04 -3.57895583e-03  4.55548640e-03\n",
      "  -1.93166006e-03 -9.97888556e-04 -6.07582625e-03 -1.69732953e-03\n",
      "   3.38058912e-03  6.09297508e-04  6.95594404e-04 -1.18004890e-03\n",
      "  -1.10683310e-03 -3.92328124e-04  2.24212777e-03 -3.94615043e-04\n",
      "   1.64571158e-03  1.30604865e-02 -9.63235839e-03  3.53038489e-04\n",
      "  -4.46725794e-05  4.53451409e-03 -7.45004719e-03  2.53291328e-03\n",
      "  -6.89739057e-03 -3.24176404e-03 -1.03379336e-02  2.05427422e-03\n",
      "   1.46200281e-03 -1.89802525e-03  1.72290814e-03 -6.57200670e-03\n",
      "   2.48386988e-03 -1.98924473e-03 -1.07171913e-02 -1.98831920e-04\n",
      "   1.44958604e-05  2.68490734e-03 -1.52353236e-03 -3.35757362e-03\n",
      "   1.81907743e-05  6.89657452e-03  4.52572292e-03  9.67742932e-04\n",
      "  -1.24587779e-03  6.91867435e-04  6.68201979e-04  9.49061660e-03]]\n",
      "[[ 1.13863778e-03 -2.64811351e-04 -8.73826433e-04]\n",
      " [-7.22627074e-04 -1.21740023e-04  8.44367097e-04]\n",
      " [-1.78858081e-04  3.63959508e-04 -1.85101426e-04]\n",
      " [ 1.39342139e-04  6.55531735e-04 -7.94873874e-04]\n",
      " [-4.49068516e-04  2.11777054e-04  2.37291462e-04]\n",
      " [-1.26586050e-03  2.12469883e-03 -8.58838332e-04]\n",
      " [ 1.55843407e-03 -9.18026601e-04 -6.40407469e-04]\n",
      " [-5.44550468e-05  1.73367770e-03 -1.67922265e-03]\n",
      " [-1.57566154e-04 -6.23835231e-04  7.81401384e-04]\n",
      " [-2.29178524e-04  4.75931282e-04 -2.46752758e-04]\n",
      " [ 1.74028963e-04 -1.30889802e-04 -4.31391605e-05]\n",
      " [-2.82164275e-04 -3.52559881e-04  6.34724156e-04]\n",
      " [ 1.60759270e-03 -1.46754752e-03 -1.40045177e-04]\n",
      " [-1.68729038e-03  2.96975183e-03 -1.28246145e-03]\n",
      " [ 9.88770107e-05  3.41438387e-04 -4.40315398e-04]\n",
      " [ 2.06222418e-04 -8.33144883e-04  6.26922465e-04]\n",
      " [-2.11268064e-03  3.05987399e-03 -9.47193351e-04]\n",
      " [ 5.20224270e-04 -6.58413700e-04  1.38189430e-04]\n",
      " [-2.80604865e-03  4.06738740e-03 -1.26133875e-03]\n",
      " [-1.26994377e-03  1.87200879e-03 -6.02065017e-04]\n",
      " [ 6.97952997e-04 -2.53331415e-04 -4.44621582e-04]\n",
      " [-1.74350534e-03  3.13737785e-03 -1.39387251e-03]\n",
      " [-1.19195736e-03  1.59731826e-03 -4.05360900e-04]\n",
      " [-1.40009753e-03  2.80879392e-03 -1.40869639e-03]\n",
      " [ 2.15500370e-03 -2.75927379e-03  6.04270089e-04]\n",
      " [ 4.38945999e-04 -3.83152278e-04 -5.57937203e-05]\n",
      " [ 4.46164211e-04 -7.04812759e-04  2.58648548e-04]\n",
      " [-1.06268790e-03  1.00217603e-04  9.62470293e-04]\n",
      " [ 2.08849377e-04  5.77018920e-04 -7.85868297e-04]\n",
      " [-1.99660436e-03  2.21282849e-03 -2.16224129e-04]\n",
      " [-4.96175695e-04  2.27150786e-04  2.69024909e-04]\n",
      " [ 1.38403532e-04  1.28656414e-03 -1.42496767e-03]\n",
      " [-1.59251759e-04 -6.39339640e-04  7.98591399e-04]\n",
      " [ 1.87899210e-03 -2.19963223e-03  3.20640129e-04]\n",
      " [ 3.76855006e-03 -3.37873932e-03 -3.89810742e-04]\n",
      " [-1.50050896e-04 -6.35964986e-04  7.86015881e-04]\n",
      " [ 8.19193329e-04  8.57133479e-04 -1.67632681e-03]\n",
      " [-1.33459863e-03  1.66715461e-03 -3.32555977e-04]\n",
      " [ 1.13919306e-03 -1.39396913e-03  2.54776070e-04]\n",
      " [-6.28393647e-04 -2.68118081e-04  8.96511728e-04]\n",
      " [-2.19419450e-03  2.39789714e-03 -2.03702638e-04]\n",
      " [ 6.75395476e-04 -7.33305870e-04  5.79103946e-05]\n",
      " [ 1.17213983e-03 -1.45192984e-03  2.79790008e-04]\n",
      " [ 1.16790755e-03 -4.12671513e-04 -7.55236037e-04]\n",
      " [ 1.68863909e-04  2.89693509e-04 -4.58557418e-04]\n",
      " [-9.45561305e-04  1.51303956e-03 -5.67478260e-04]\n",
      " [-1.77079967e-03  2.71822861e-03 -9.47428934e-04]\n",
      " [ 9.43586724e-05 -7.12012786e-04  6.17654114e-04]\n",
      " [-4.66608297e-04  2.76803907e-04  1.89804389e-04]\n",
      " [-3.44409948e-04  2.24568485e-04  1.19841464e-04]\n",
      " [ 1.75391146e-03 -2.49500985e-03  7.41098388e-04]\n",
      " [ 1.97905138e-03 -1.64660396e-03 -3.32447422e-04]\n",
      " [-4.90638795e-04 -2.75025764e-04  7.65664559e-04]\n",
      " [ 8.01838165e-04 -4.68845343e-04 -3.32992822e-04]\n",
      " [ 1.51824629e-03 -2.16818045e-03  6.49934163e-04]\n",
      " [ 1.41923025e-03 -2.18351147e-03  7.64281226e-04]\n",
      " [ 2.57163297e-03 -2.19541292e-03 -3.76220045e-04]\n",
      " [ 8.24864293e-04 -1.02655981e-03  2.01695521e-04]\n",
      " [-2.58066060e-03  3.92964319e-03 -1.34898259e-03]\n",
      " [-2.37811858e-04 -9.33354188e-05  3.31147277e-04]\n",
      " [-3.57817849e-04  7.22009254e-05  2.85616924e-04]\n",
      " [ 3.03580711e-05 -9.23330400e-05  6.19749688e-05]\n",
      " [-1.71468939e-04 -7.09455200e-04  8.80924139e-04]\n",
      " [-3.37139065e-03  4.69634740e-03 -1.32495675e-03]]\n",
      "[[-0.00390034  0.00450921 -0.00060886]]\n"
     ]
    }
   ],
   "source": [
    "loss_activation.backward(loss_activation.output, Y)\n",
    "layer2.backward(loss_activation.dinputs)\n",
    "activation1.backward(layer2.dinputs)\n",
    "layer1.backward(activation1.dinputs)\n",
    "\n",
    "\n",
    "\n",
    "print(layer1.dweights)\n",
    "print(layer1.dbiases)\n",
    "print(layer2.dweights)\n",
    "print(layer2.dbiases)\n",
    "\n",
    "optimizer.update_params(layer1)\n",
    "optimizer.update_params(layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a582f6",
   "metadata": {},
   "source": [
    "# Learning with the SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b22ad357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | acc: 0.360 | loss: 1.097\n",
      "Epoch: 500 | acc: 0.413 | loss: 1.096\n",
      "Epoch: 1000 | acc: 0.397 | loss: 1.095\n",
      "Epoch: 1500 | acc: 0.390 | loss: 1.094\n",
      "Epoch: 2000 | acc: 0.390 | loss: 1.094\n",
      "Epoch: 2500 | acc: 0.400 | loss: 1.093\n",
      "Epoch: 3000 | acc: 0.400 | loss: 1.092\n",
      "Epoch: 3500 | acc: 0.413 | loss: 1.092\n",
      "Epoch: 4000 | acc: 0.417 | loss: 1.091\n",
      "Epoch: 4500 | acc: 0.417 | loss: 1.091\n",
      "Epoch: 5000 | acc: 0.417 | loss: 1.091\n",
      "Epoch: 5500 | acc: 0.417 | loss: 1.090\n",
      "Epoch: 6000 | acc: 0.420 | loss: 1.090\n",
      "Epoch: 6500 | acc: 0.420 | loss: 1.089\n",
      "Epoch: 7000 | acc: 0.423 | loss: 1.089\n",
      "Epoch: 7500 | acc: 0.423 | loss: 1.089\n",
      "Epoch: 8000 | acc: 0.420 | loss: 1.088\n",
      "Epoch: 8500 | acc: 0.420 | loss: 1.088\n",
      "Epoch: 9000 | acc: 0.420 | loss: 1.088\n",
      "Epoch: 9500 | acc: 0.420 | loss: 1.087\n",
      "Epoch: 10000 | acc: 0.420 | loss: 1.087\n",
      "Epoch: 10500 | acc: 0.417 | loss: 1.087\n",
      "Epoch: 11000 | acc: 0.417 | loss: 1.087\n",
      "Epoch: 11500 | acc: 0.413 | loss: 1.086\n",
      "Epoch: 12000 | acc: 0.417 | loss: 1.086\n",
      "Epoch: 12500 | acc: 0.417 | loss: 1.086\n",
      "Epoch: 13000 | acc: 0.420 | loss: 1.086\n",
      "Epoch: 13500 | acc: 0.423 | loss: 1.085\n",
      "Epoch: 14000 | acc: 0.423 | loss: 1.085\n",
      "Epoch: 14500 | acc: 0.423 | loss: 1.085\n",
      "Epoch: 15000 | acc: 0.423 | loss: 1.085\n",
      "Epoch: 15500 | acc: 0.427 | loss: 1.084\n",
      "Epoch: 16000 | acc: 0.427 | loss: 1.084\n",
      "Epoch: 16500 | acc: 0.427 | loss: 1.084\n",
      "Epoch: 17000 | acc: 0.427 | loss: 1.084\n",
      "Epoch: 17500 | acc: 0.427 | loss: 1.083\n",
      "Epoch: 18000 | acc: 0.427 | loss: 1.083\n",
      "Epoch: 18500 | acc: 0.427 | loss: 1.083\n",
      "Epoch: 19000 | acc: 0.427 | loss: 1.083\n",
      "Epoch: 19500 | acc: 0.427 | loss: 1.083\n",
      "Epoch: 20000 | acc: 0.423 | loss: 1.082\n",
      "Epoch: 20500 | acc: 0.423 | loss: 1.082\n",
      "Epoch: 21000 | acc: 0.427 | loss: 1.082\n",
      "Epoch: 21500 | acc: 0.427 | loss: 1.082\n",
      "Epoch: 22000 | acc: 0.427 | loss: 1.082\n",
      "Epoch: 22500 | acc: 0.427 | loss: 1.082\n",
      "Epoch: 23000 | acc: 0.427 | loss: 1.081\n",
      "Epoch: 23500 | acc: 0.427 | loss: 1.081\n",
      "Epoch: 24000 | acc: 0.423 | loss: 1.081\n",
      "Epoch: 24500 | acc: 0.427 | loss: 1.081\n",
      "Epoch: 25000 | acc: 0.427 | loss: 1.081\n",
      "Epoch: 25500 | acc: 0.430 | loss: 1.081\n",
      "Epoch: 26000 | acc: 0.430 | loss: 1.080\n",
      "Epoch: 26500 | acc: 0.433 | loss: 1.080\n",
      "Epoch: 27000 | acc: 0.433 | loss: 1.080\n",
      "Epoch: 27500 | acc: 0.433 | loss: 1.080\n",
      "Epoch: 28000 | acc: 0.433 | loss: 1.080\n",
      "Epoch: 28500 | acc: 0.433 | loss: 1.080\n",
      "Epoch: 29000 | acc: 0.430 | loss: 1.080\n",
      "Epoch: 29500 | acc: 0.430 | loss: 1.080\n",
      "Epoch: 30000 | acc: 0.430 | loss: 1.079\n",
      "Epoch: 30500 | acc: 0.430 | loss: 1.079\n",
      "Epoch: 31000 | acc: 0.430 | loss: 1.079\n",
      "Epoch: 31500 | acc: 0.430 | loss: 1.079\n",
      "Epoch: 32000 | acc: 0.430 | loss: 1.079\n",
      "Epoch: 32500 | acc: 0.430 | loss: 1.079\n",
      "Epoch: 33000 | acc: 0.430 | loss: 1.079\n",
      "Epoch: 33500 | acc: 0.430 | loss: 1.079\n",
      "Epoch: 34000 | acc: 0.430 | loss: 1.079\n",
      "Epoch: 34500 | acc: 0.430 | loss: 1.078\n",
      "Epoch: 35000 | acc: 0.430 | loss: 1.078\n",
      "Epoch: 35500 | acc: 0.430 | loss: 1.078\n",
      "Epoch: 36000 | acc: 0.430 | loss: 1.078\n",
      "Epoch: 36500 | acc: 0.433 | loss: 1.078\n",
      "Epoch: 37000 | acc: 0.433 | loss: 1.078\n",
      "Epoch: 37500 | acc: 0.433 | loss: 1.078\n",
      "Epoch: 38000 | acc: 0.433 | loss: 1.078\n",
      "Epoch: 38500 | acc: 0.433 | loss: 1.078\n",
      "Epoch: 39000 | acc: 0.433 | loss: 1.078\n",
      "Epoch: 39500 | acc: 0.433 | loss: 1.078\n",
      "Epoch: 40000 | acc: 0.433 | loss: 1.078\n",
      "Epoch: 40500 | acc: 0.433 | loss: 1.078\n",
      "Epoch: 41000 | acc: 0.433 | loss: 1.077\n",
      "Epoch: 41500 | acc: 0.433 | loss: 1.077\n",
      "Epoch: 42000 | acc: 0.433 | loss: 1.077\n",
      "Epoch: 42500 | acc: 0.433 | loss: 1.077\n",
      "Epoch: 43000 | acc: 0.433 | loss: 1.077\n",
      "Epoch: 43500 | acc: 0.433 | loss: 1.077\n",
      "Epoch: 44000 | acc: 0.430 | loss: 1.077\n",
      "Epoch: 44500 | acc: 0.423 | loss: 1.077\n",
      "Epoch: 45000 | acc: 0.423 | loss: 1.077\n",
      "Epoch: 45500 | acc: 0.423 | loss: 1.077\n",
      "Epoch: 46000 | acc: 0.423 | loss: 1.077\n",
      "Epoch: 46500 | acc: 0.423 | loss: 1.077\n",
      "Epoch: 47000 | acc: 0.423 | loss: 1.077\n",
      "Epoch: 47500 | acc: 0.423 | loss: 1.077\n",
      "Epoch: 48000 | acc: 0.423 | loss: 1.077\n",
      "Epoch: 48500 | acc: 0.423 | loss: 1.077\n",
      "Epoch: 49000 | acc: 0.423 | loss: 1.077\n",
      "Epoch: 49500 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 50000 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 50500 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 51000 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 51500 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 52000 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 52500 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 53000 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 53500 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 54000 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 54500 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 55000 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 55500 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 56000 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 56500 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 57000 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 57500 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 58000 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 58500 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 59000 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 59500 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 60000 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 60500 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 61000 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 61500 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 62000 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 62500 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 63000 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 63500 | acc: 0.423 | loss: 1.076\n",
      "Epoch: 64000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 64500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 65000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 65500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 66000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 66500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 67000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 67500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 68000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 68500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 69000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 69500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 70000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 70500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 71000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 71500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 72000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 72500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 73000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 73500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 74000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 74500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 75000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 75500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 76000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 76500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 77000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 77500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 78000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 78500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 79000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 79500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 80000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 80500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 81000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 81500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 82000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 82500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 83000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 83500 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 84000 | acc: 0.423 | loss: 1.075\n",
      "Epoch: 84500 | acc: 0.427 | loss: 1.074\n",
      "Epoch: 85000 | acc: 0.427 | loss: 1.074\n",
      "Epoch: 85500 | acc: 0.427 | loss: 1.074\n",
      "Epoch: 86000 | acc: 0.427 | loss: 1.074\n",
      "Epoch: 86500 | acc: 0.427 | loss: 1.074\n",
      "Epoch: 87000 | acc: 0.427 | loss: 1.074\n",
      "Epoch: 87500 | acc: 0.427 | loss: 1.074\n",
      "Epoch: 88000 | acc: 0.427 | loss: 1.074\n",
      "Epoch: 88500 | acc: 0.427 | loss: 1.074\n",
      "Epoch: 89000 | acc: 0.427 | loss: 1.074\n",
      "Epoch: 89500 | acc: 0.427 | loss: 1.074\n",
      "Epoch: 90000 | acc: 0.427 | loss: 1.074\n",
      "Epoch: 90500 | acc: 0.427 | loss: 1.074\n",
      "Epoch: 91000 | acc: 0.427 | loss: 1.074\n",
      "Epoch: 91500 | acc: 0.423 | loss: 1.074\n",
      "Epoch: 92000 | acc: 0.423 | loss: 1.074\n",
      "Epoch: 92500 | acc: 0.423 | loss: 1.074\n",
      "Epoch: 93000 | acc: 0.423 | loss: 1.074\n",
      "Epoch: 93500 | acc: 0.423 | loss: 1.074\n",
      "Epoch: 94000 | acc: 0.423 | loss: 1.074\n",
      "Epoch: 94500 | acc: 0.423 | loss: 1.074\n",
      "Epoch: 95000 | acc: 0.423 | loss: 1.074\n",
      "Epoch: 95500 | acc: 0.423 | loss: 1.074\n",
      "Epoch: 96000 | acc: 0.423 | loss: 1.074\n",
      "Epoch: 96500 | acc: 0.423 | loss: 1.074\n",
      "Epoch: 97000 | acc: 0.420 | loss: 1.074\n",
      "Epoch: 97500 | acc: 0.420 | loss: 1.074\n",
      "Epoch: 98000 | acc: 0.420 | loss: 1.074\n",
      "Epoch: 98500 | acc: 0.420 | loss: 1.074\n",
      "Epoch: 99000 | acc: 0.420 | loss: 1.074\n",
      "Epoch: 99500 | acc: 0.420 | loss: 1.074\n",
      "Epoch: 100000 | acc: 0.420 | loss: 1.074\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100001):\n",
    "    layer1.forward(X)\n",
    "    activation1.forward(layer1.output)\n",
    "    layer2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(layer2.output, Y)\n",
    "    predictions = np.argmax(loss_activation.output,axis=1)\n",
    "    if len(Y.shape)==2:\n",
    "        Y = np.argmax(Y,axis=1)\n",
    "    accuracy = np.mean(predictions==Y)\n",
    "    \n",
    "    if epoch%500==0:\n",
    "        print(f\"Epoch: {epoch} | acc: {accuracy:.3f} | loss: {loss:.3f}\")\n",
    "    \n",
    "    loss_activation.backward(loss_activation.output, Y)\n",
    "    layer2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(layer2.dinputs)\n",
    "    layer1.backward(activation1.dinputs)\n",
    "    \n",
    "    optimizer.update_params(layer1)\n",
    "    optimizer.update_params(layer2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
