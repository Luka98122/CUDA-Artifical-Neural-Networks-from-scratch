{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e46097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "from ctypes import c_float, POINTER, c_int\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Build command (rtx 4090)\n",
    "# cd C:\\Users\\luka\\source\\repos\\Artifical-Neural-Networks-From-Scratch\\cuda\\main\n",
    "# nvcc -shared math_ops.cu nn_layers.cu -o nn.dll -arch=sm_89 -Xcompiler \"/MD\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33abc329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CudaNN:\n",
    "    def __init__(self, dll_path=\"./main/nn.dll\"):\n",
    "        if not os.path.exists(dll_path):\n",
    "            raise FileNotFoundError(f\"DLL not found at {dll_path}\")\n",
    "            \n",
    "        self.lib = ctypes.CDLL(dll_path)\n",
    "        \n",
    "        # Args and Outputs initialization\n",
    "        \n",
    "        self.lib.mat_add_cuda.argtypes = [\n",
    "            POINTER(c_float), POINTER(c_float), POINTER(c_float),\n",
    "            c_int, c_int\n",
    "        ]\n",
    "        self.lib.mat_add_cuda.restype = None\n",
    "\n",
    "        self.lib.dot_product_cuda.argtypes = [\n",
    "            POINTER(c_float), POINTER(c_float), POINTER(c_float), c_int\n",
    "        ]\n",
    "        self.lib.dot_product_cuda.restype = None\n",
    "\n",
    "        self.lib.forward_layer_cuda.argtypes = [\n",
    "            POINTER(c_float), POINTER(c_float), POINTER(c_float), POINTER(c_float),\n",
    "            c_int, c_int\n",
    "        ]\n",
    "        self.lib.forward_layer_cuda.restype = None\n",
    "\n",
    "    def mat_add(self, A, B):\n",
    "        rows, cols = A.shape\n",
    "        C = np.zeros_like(A, dtype=np.float32)\n",
    "        \n",
    "        self.lib.mat_add_cuda(\n",
    "            A.ctypes.data_as(POINTER(c_float)),\n",
    "            B.ctypes.data_as(POINTER(c_float)),\n",
    "            C.ctypes.data_as(POINTER(c_float)),\n",
    "            rows, cols\n",
    "        )\n",
    "        return C\n",
    "\n",
    "    def dot_product(self, a, b):\n",
    "        n = a.size\n",
    "        res = c_float()\n",
    "        \n",
    "        self.lib.dot_product_cuda(\n",
    "            a.ctypes.data_as(POINTER(c_float)),\n",
    "            b.ctypes.data_as(POINTER(c_float)),\n",
    "            ctypes.byref(res),\n",
    "            n\n",
    "        )\n",
    "        return res.value\n",
    "\n",
    "    def forward_layer(self, inputs, weights, bias, n_out):\n",
    "        n_in = inputs.size\n",
    "        output = np.zeros(n_out, dtype=np.float32)\n",
    "        \n",
    "        self.lib.forward_layer_cuda(\n",
    "            inputs.ctypes.data_as(POINTER(c_float)),\n",
    "            weights.ctypes.data_as(POINTER(c_float)),\n",
    "            bias.ctypes.data_as(POINTER(c_float)),\n",
    "            output.ctypes.data_as(POINTER(c_float)),\n",
    "            n_in, n_out\n",
    "        )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e3e22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_nn = CudaNN(\"./main/nn.dll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e32c79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Add time: 0.0542s\n"
     ]
    }
   ],
   "source": [
    "# --- Test 1: Matrix Addition ---\n",
    "rows, cols = 5120, 5120\n",
    "A = np.random.rand(rows, cols).astype(np.float32)\n",
    "B = np.random.rand(rows, cols).astype(np.float32)\n",
    "\n",
    "start = time.time()\n",
    "C = cuda_nn.mat_add(A, B) \n",
    "print(f\"CUDA Add time: {time.time() - start:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b253f984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product: 30720.0\n"
     ]
    }
   ],
   "source": [
    "# --- Test 2: Dot Product ---\n",
    "n = 10240\n",
    "vec_a = np.ones(n, dtype=np.float32)\n",
    "vec_b = np.ones(n, dtype=np.float32) * 3.0\n",
    "\n",
    "dot_res = cuda_nn.dot_product(vec_a, vec_b)\n",
    "print(f\"Dot product: {dot_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f5f9bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA:  [ 3.1  7.2 11.3]\n",
      "Numpy: [ 3.1  7.2 11.3]\n",
      "Success: CUDA matches NumPy\n"
     ]
    }
   ],
   "source": [
    "# --- Test 3: Forward Layer ---\n",
    "# 4 inputs -> 3 outputs\n",
    "input_vec = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n",
    "# Flattened weights (3 neurons * 4 weights each)\n",
    "weights = np.array([\n",
    "    0.1, 0.2, 0.3, 0.4, \n",
    "    0.5, 0.6, 0.7, 0.8, \n",
    "    0.9, 1.0, 1.1, 1.2\n",
    "], dtype=np.float32)\n",
    "bias = np.array([0.1, 0.2, 0.3], dtype=np.float32)\n",
    "\n",
    "cuda_out = cuda_nn.forward_layer(input_vec, weights, bias, n_out=3)\n",
    "\n",
    "# Numpy verification\n",
    "numpy_out = np.dot(weights.reshape(3, 4), input_vec) + bias\n",
    "\n",
    "print(f\"CUDA:  {cuda_out}\")\n",
    "print(f\"Numpy: {numpy_out}\")\n",
    "\n",
    "if np.allclose(cuda_out, numpy_out, atol=1e-5):\n",
    "    print(\"Success: CUDA matches NumPy\")\n",
    "else:\n",
    "    print(\"Error: Results do not match\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
