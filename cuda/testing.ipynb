{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e46097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "from ctypes import c_float, POINTER, c_int\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import platform\n",
    "\n",
    "# Build command (rtx 4090), windows\n",
    "# cd C:\\Users\\luka\\source\\repos\\Artifical-Neural-Networks-From-Scratch\\cuda\\main\n",
    "# nvcc -shared math_ops.cu nn_activations.cu nn_layers.cu -o nn.dll -arch=sm_89 -Xcompiler \"/MD\"\n",
    "\n",
    "# Linux: \n",
    "# nvcc -shared ./cuda/main/math_ops.cu ./cuda/main/nn_layers.cu ./cuda/main/nn_activations.cu -o ./cuda/main/libnn.so -arch=sm_86 -Xcompiler -fPIC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33abc329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CudaNN:\n",
    "    def __init__(self, dll_path=\"./main/nn.dll\"):\n",
    "        lib_path = None\n",
    "        if lib_path is None:\n",
    "            if platform.system() == \"Windows\":\n",
    "                lib_path = \"./main/nn.dll\"       # Windows path\n",
    "            else:\n",
    "                lib_path = \"./main/libnn.so\" # Linux path\n",
    "                \n",
    "        # Jupyter runs from the directory the .ipynb file is in. \n",
    "        # Make sure this relative path is correct!\n",
    "        if not os.path.exists(lib_path):\n",
    "            raise FileNotFoundError(f\"Library not found at {lib_path}. Check your notebook's current working directory: {os.getcwd()}\")\n",
    "        self.lib = ctypes.CDLL(dll_path)\n",
    "        \n",
    "        # Args and Outputs initialization\n",
    "        \n",
    "        self.lib.mat_add_cuda.argtypes = [\n",
    "            POINTER(c_float), POINTER(c_float), POINTER(c_float),\n",
    "            c_int, c_int\n",
    "        ]\n",
    "        self.lib.mat_add_cuda.restype = None\n",
    "\n",
    "\n",
    "        self.lib.dot_product_cuda.argtypes = [\n",
    "            POINTER(c_float), POINTER(c_float), POINTER(c_float), c_int\n",
    "        ]\n",
    "        self.lib.dot_product_cuda.restype = None\n",
    "\n",
    "\n",
    "        self.lib.forward_layer_cuda.argtypes = [\n",
    "            POINTER(c_float), POINTER(c_float), POINTER(c_float), POINTER(c_float),\n",
    "            c_int, c_int\n",
    "        ]\n",
    "        self.lib.forward_layer_cuda.restype = None\n",
    "\n",
    "\n",
    "        self.lib.argmax_cuda.argtypes = [\n",
    "            POINTER(c_float), POINTER(c_int), c_int\n",
    "        ]\n",
    "        self.lib.argmax_cuda.restype = None\n",
    "\n",
    "\n",
    "        self.lib.relu_cuda.argtypes = [\n",
    "            POINTER(c_float), c_int\n",
    "        ]\n",
    "        self.lib.relu_cuda.restype = None\n",
    "\n",
    "        self.lib.relu_cuda_100.argtypes = [\n",
    "            POINTER(c_float), c_int\n",
    "        ]\n",
    "        self.lib.relu_cuda_100.restype = None\n",
    "\n",
    "    def relu(self, arr):\n",
    "        n = arr.size\n",
    "        self.lib.relu_cuda(\n",
    "            arr.ctypes.data_as(POINTER(c_float)),\n",
    "            n\n",
    "        )\n",
    "        return arr\n",
    "\n",
    "    def relu_100(self, arr):\n",
    "        n = arr.size\n",
    "        self.lib.relu_cuda_100(\n",
    "            arr.ctypes.data_as(POINTER(c_float)),\n",
    "            n\n",
    "        )\n",
    "        return arr\n",
    "\n",
    "    def mat_add(self, A, B):\n",
    "        rows, cols = A.shape\n",
    "        C = np.zeros_like(A, dtype=np.float32)\n",
    "        \n",
    "        self.lib.mat_add_cuda(\n",
    "            A.ctypes.data_as(POINTER(c_float)),\n",
    "            B.ctypes.data_as(POINTER(c_float)),\n",
    "            C.ctypes.data_as(POINTER(c_float)),\n",
    "            rows, cols\n",
    "        )\n",
    "        return C\n",
    "\n",
    "    def argmax(self, arr):\n",
    "        n = arr.size\n",
    "        result = c_int()\n",
    "        \n",
    "        self.lib.argmax_cuda(\n",
    "            arr.ctypes.data_as(POINTER(c_float)),\n",
    "            ctypes.byref(result),\n",
    "            n\n",
    "        )\n",
    "        return result.value\n",
    "\n",
    "    def dot_product(self, a, b):\n",
    "        n = a.size\n",
    "        res = c_float()\n",
    "        \n",
    "        self.lib.dot_product_cuda(\n",
    "            a.ctypes.data_as(POINTER(c_float)),\n",
    "            b.ctypes.data_as(POINTER(c_float)),\n",
    "            ctypes.byref(res),\n",
    "            n\n",
    "        )\n",
    "        return res.value\n",
    "\n",
    "    def forward_layer(self, inputs, weights, bias, n_out):\n",
    "        n_in = inputs.size\n",
    "        output = np.zeros(n_out, dtype=np.float32)\n",
    "        \n",
    "        self.lib.forward_layer_cuda(\n",
    "            inputs.ctypes.data_as(POINTER(c_float)),\n",
    "            weights.ctypes.data_as(POINTER(c_float)),\n",
    "            bias.ctypes.data_as(POINTER(c_float)),\n",
    "            output.ctypes.data_as(POINTER(c_float)),\n",
    "            n_in, n_out\n",
    "        )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e3e22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_nn = CudaNN(\"./main/libnn.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e32c79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Add time: 0.0663s\n"
     ]
    }
   ],
   "source": [
    "# --- Test 1: Matrix Addition ---\n",
    "rows, cols = 5120, 5120\n",
    "A = np.random.rand(rows, cols).astype(np.float32)\n",
    "B = np.random.rand(rows, cols).astype(np.float32)\n",
    "\n",
    "start = time.time()\n",
    "C = cuda_nn.mat_add(A, B) \n",
    "print(f\"CUDA Add time: {time.time() - start:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b253f984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product: 30720.0\n"
     ]
    }
   ],
   "source": [
    "# --- Test 2: Dot Product ---\n",
    "n = 10240\n",
    "vec_a = np.ones(n, dtype=np.float32)\n",
    "vec_b = np.ones(n, dtype=np.float32) * 3.0\n",
    "\n",
    "dot_res = cuda_nn.dot_product(vec_a, vec_b)\n",
    "print(f\"Dot product: {dot_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f5f9bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA:  [ 3.1  7.2 11.3]\n",
      "Numpy: [ 3.1  7.2 11.3]\n",
      "Success: CUDA matches NumPy\n"
     ]
    }
   ],
   "source": [
    "# --- Test 3: Forward Layer ---\n",
    "# 4 inputs -> 3 outputs\n",
    "input_vec = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n",
    "# Flattened weights (3 neurons * 4 weights each)\n",
    "weights = np.array([\n",
    "    0.1, 0.2, 0.3, 0.4, \n",
    "    0.5, 0.6, 0.7, 0.8, \n",
    "    0.9, 1.0, 1.1, 1.2\n",
    "], dtype=np.float32)\n",
    "bias = np.array([0.1, 0.2, 0.3], dtype=np.float32)\n",
    "\n",
    "cuda_out = cuda_nn.forward_layer(input_vec, weights, bias, n_out=3)\n",
    "\n",
    "# Numpy verification\n",
    "numpy_out = np.dot(weights.reshape(3, 4), input_vec) + bias\n",
    "\n",
    "print(f\"CUDA:  {cuda_out}\")\n",
    "print(f\"Numpy: {numpy_out}\")\n",
    "\n",
    "if np.allclose(cuda_out, numpy_out, atol=1e-5):\n",
    "    print(\"Success: CUDA matches NumPy\")\n",
    "else:\n",
    "    print(\"Error: Results do not match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56fc8dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Argmax index:  2 (Value: 0.60)\n",
      "Numpy Argmax index: 2 (Value: 0.60)\n",
      "Success: CUDA matches NumPy\n"
     ]
    }
   ],
   "source": [
    "# --- Test 4: Argmax ---\n",
    "# Simulating a neural network's probability output for 5 classes\n",
    "probs = np.array([0.05, 0.15, 0.60, 0.10, 0.10], dtype=np.float32)\n",
    "\n",
    "cuda_idx = cuda_nn.argmax(probs)\n",
    "numpy_idx = np.argmax(probs)\n",
    "\n",
    "print(f\"CUDA Argmax index:  {cuda_idx} (Value: {probs[cuda_idx]:.2f})\")\n",
    "print(f\"Numpy Argmax index: {numpy_idx} (Value: {probs[numpy_idx]:.2f})\")\n",
    "\n",
    "if cuda_idx == numpy_idx:\n",
    "    print(\"Success: CUDA matches NumPy\")\n",
    "else:\n",
    "    print(\"Error: Results do not match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c7979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 10,000,000 elements:\n",
      "Method          | Time (s)     | Speedup vs Python \n",
      "--------------------------------------------------\n",
      "Raw Python      | 0.395161     | 1.0x              \n",
      "NumPy           | 0.007542     | 52.40x\n",
      "NumPy (x100)    | 0.998580     | 0.40x\n",
      "CUDA (Inc. Mem) | 0.014280     | 27.67x\n",
      "CUDA (x100)     | 0.023807     | 16.60x\n",
      "\n",
      "✅ Verification Successful: CUDA matches NumPy\n"
     ]
    }
   ],
   "source": [
    "# --- Test 5: ReLU Activation ---\n",
    "# Array with a mix of positive and negative numbers\n",
    "\n",
    "# 1,000,000 elements\n",
    "N = 10_000_000\n",
    "test_data = np.random.uniform(-10, 10, N).astype(np.float32)\n",
    "\n",
    "# --- 1. Raw Python ---\n",
    "python_list = test_data.tolist()\n",
    "start = time.time()\n",
    "python_res = [x if x > 0 else 0.0 for x in python_list]\n",
    "python_time = time.time() - start\n",
    "\n",
    "# --- 2. NumPy ---\n",
    "start = time.time()\n",
    "numpy_res = np.maximum(0, test_data)\n",
    "numpy_time = time.time() - start\n",
    "\n",
    "# --- 3. NumPy x100 ---\n",
    "start = time.time()\n",
    "for i in range(100): # This is to attempt to combat the overhead of CUDA when benchmarking\n",
    "    numpy_res = np.maximum(0, test_data)\n",
    "numpy_time_100 = time.time() - start\n",
    "\n",
    "# --- 4. CUDA Kernel ---\n",
    "cuda_data = test_data.copy()\n",
    "start = time.time()\n",
    "cuda_nn.relu(cuda_data)\n",
    "cuda_time = time.time() - start\n",
    "\n",
    "# --- 5. CUDA Kernel x100 ---\n",
    "cuda_data_100 = test_data.copy()\n",
    "start = time.time()\n",
    "cuda_nn.relu_100(cuda_data_100)\n",
    "cuda_time_100 = time.time() - start\n",
    "\n",
    "# --- Results ---\n",
    "print(f\"Results for {N:,} elements:\")\n",
    "print(f\"{'Method':<15} | {'Time (s)':<12} | {'Speedup vs Python':<18}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Raw Python':<15} | {python_time:<12.6f} | {'1.0x':<18}\")\n",
    "print(f\"{'NumPy':<15} | {numpy_time:<12.6f} | {python_time/numpy_time:.2f}x\")\n",
    "print(f\"{'NumPy (x100)':<15} | {numpy_time_100:<12.6f} | {python_time/numpy_time_100:.2f}x\")\n",
    "print(f\"{'CUDA (Inc. Mem)':<15} | {cuda_time:<12.6f} | {python_time/cuda_time:.2f}x\")\n",
    "print(f\"{'CUDA (x100)':<15} | {cuda_time_100:<12.6f} | {python_time/cuda_time_100:.2f}x\")\n",
    "\n",
    "# Verification\n",
    "if np.allclose(cuda_data, numpy_res):\n",
    "    print(\"\\n✅ Verification Successful: CUDA matches NumPy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
