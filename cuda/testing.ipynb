{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e46097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "from ctypes import c_void_p, c_int, POINTER, c_float\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import platform\n",
    "\n",
    "# Build command (rtx 4090), windows\n",
    "# cd C:\\Users\\luka\\source\\repos\\Artifical-Neural-Networks-From-Scratch\\cuda\\main\n",
    "# nvcc -shared math_ops.cu nn_activations.cu nn_layers.cu -o nn.dll -arch=sm_89 -Xcompiler \"/MD\"\n",
    "\n",
    "# Linux: \n",
    "# nvcc -shared ./cuda/main/math_ops.cu ./cuda/main/nn_layers.cu ./cuda/main/nn_activations.cu -o ./cuda/main/libnn.so -arch=sm_86 -Xcompiler -fPIC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33abc329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CudaNN:\n",
    "    def __init__(self, dll_path=\"./main/nn.dll\"):\n",
    "        lib_path = None\n",
    "        if lib_path is None:\n",
    "            if platform.system() == \"Windows\":\n",
    "                lib_path = \"./main/nn.dll\"       # Windows path\n",
    "            else:\n",
    "                lib_path = \"./main/libnn.so\" # Linux path\n",
    "                \n",
    "        # Jupyter runs from the directory the .ipynb file is in. \n",
    "        # Make sure this relative path is correct!\n",
    "        if not os.path.exists(lib_path):\n",
    "            raise FileNotFoundError(f\"Library not found at {lib_path}. Check your notebook's current working directory: {os.getcwd()}\")\n",
    "        self.lib = ctypes.CDLL(dll_path)\n",
    "        \n",
    "        # Args and Outputs initialization\n",
    "        \n",
    "        self.lib.mat_add_cuda.argtypes = [\n",
    "            POINTER(c_float), POINTER(c_float), POINTER(c_float),\n",
    "            c_int, c_int\n",
    "        ]\n",
    "        self.lib.mat_add_cuda.restype = None\n",
    "\n",
    "\n",
    "        self.lib.dot_product_cuda.argtypes = [\n",
    "            POINTER(c_float), POINTER(c_float), POINTER(c_float), c_int\n",
    "        ]\n",
    "        self.lib.dot_product_cuda.restype = None\n",
    "\n",
    "        self.lib.argmax_cuda.argtypes = [\n",
    "            POINTER(c_float), POINTER(c_int), c_int\n",
    "        ]\n",
    "        self.lib.argmax_cuda.restype = None\n",
    "\n",
    "\n",
    "        self.lib.relu_cuda.argtypes = [\n",
    "            POINTER(c_float), c_int\n",
    "        ]\n",
    "        self.lib.relu_cuda.restype = None\n",
    "\n",
    "        self.lib.relu_cuda_100.argtypes = [\n",
    "            POINTER(c_float), c_int\n",
    "        ]\n",
    "        self.lib.relu_cuda_100.restype = None\n",
    "\n",
    "        self.lib.create_dense_layer.argtypes = [c_int, c_int, POINTER(c_float), POINTER(c_float)]\n",
    "        self.lib.create_dense_layer.restype = c_void_p # Returns a memory address\n",
    "\n",
    "        self.lib.forward_dense_layer.argtypes = [c_void_p, c_void_p] # Takes layer pointer and device input pointer\n",
    "        self.lib.forward_dense_layer.restype = c_void_p # Returns device output pointer\n",
    "\n",
    "        self.lib.destroy_dense_layer.argtypes = [c_void_p]\n",
    "        self.lib.destroy_dense_layer.restype = None\n",
    "\n",
    "        self.lib.forward_dense_layer_host.argtypes = [c_void_p, POINTER(c_float), POINTER(c_float)]\n",
    "        self.lib.forward_dense_layer_host.restype = None\n",
    "\n",
    "        self.lib.copy_array_to_device.argtypes = [POINTER(c_float),c_int]\n",
    "        self.lib.copy_array_to_device.restype = c_void_p\n",
    "\n",
    "        self.lib.copy_device_array_to_host.argtypes = [c_void_p,c_int]\n",
    "        self.lib.copy_device_array_to_host.restype = c_void_p\n",
    "\n",
    "    def relu(self, arr):\n",
    "        n = arr.size\n",
    "        self.lib.relu_cuda(\n",
    "            arr.ctypes.data_as(POINTER(c_float)),\n",
    "            n\n",
    "        )\n",
    "        return arr\n",
    "\n",
    "    def relu_100(self, arr):\n",
    "        n = arr.size\n",
    "        self.lib.relu_cuda_100(\n",
    "            arr.ctypes.data_as(POINTER(c_float)),\n",
    "            n\n",
    "        )\n",
    "        return arr\n",
    "\n",
    "    def mat_add(self, A, B):\n",
    "        rows, cols = A.shape\n",
    "        C = np.zeros_like(A, dtype=np.float32)\n",
    "        \n",
    "        self.lib.mat_add_cuda(\n",
    "            A.ctypes.data_as(POINTER(c_float)),\n",
    "            B.ctypes.data_as(POINTER(c_float)),\n",
    "            C.ctypes.data_as(POINTER(c_float)),\n",
    "            rows, cols\n",
    "        )\n",
    "        return C\n",
    "\n",
    "    def argmax(self, arr):\n",
    "        n = arr.size\n",
    "        result = c_int()\n",
    "        \n",
    "        self.lib.argmax_cuda(\n",
    "            arr.ctypes.data_as(POINTER(c_float)),\n",
    "            ctypes.byref(result),\n",
    "            n\n",
    "        )\n",
    "        return result.value\n",
    "\n",
    "    def dot_product(self, a, b):\n",
    "        n = a.size\n",
    "        res = c_float()\n",
    "        \n",
    "        self.lib.dot_product_cuda(\n",
    "            a.ctypes.data_as(POINTER(c_float)),\n",
    "            b.ctypes.data_as(POINTER(c_float)),\n",
    "            ctypes.byref(res),\n",
    "            n\n",
    "        )\n",
    "        return res.value\n",
    "    \n",
    "    \n",
    "    class DenseLayer:\n",
    "        def __init__(self, lib, weights, bias):\n",
    "            self.lib = lib\n",
    "            self.n_outputs, self.n_inputs = weights.shape \n",
    "        \n",
    "            self.obj_ptr = self.lib.create_dense_layer(\n",
    "                self.n_inputs, self.n_outputs, \n",
    "                weights.ctypes.data_as(POINTER(c_float)), \n",
    "                bias.ctypes.data_as(POINTER(c_float))\n",
    "            )\n",
    "            \n",
    "        def forward(self, d_inputs):\n",
    "            d_ptr = self.lib.forward_dense_layer(\n",
    "                self.obj_ptr,\n",
    "                d_inputs\n",
    "            )\n",
    "            return d_ptr\n",
    "            \n",
    "        def __del__(self): # prevents memory leak\n",
    "            try:\n",
    "                if self.obj_ptr:\n",
    "                    self.lib.destroy_dense_layer(self.obj_ptr)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    def copy_array_to_device(self,arr):\n",
    "        return self.lib.copy_array_to_device(arr.ctypes.data_as(POINTER(c_float)),arr.size)\n",
    "\n",
    "    def copy_device_array_to_host(self, d_arr, layer_obj: DenseLayer):\n",
    "        h_ptr = self.lib.copy_device_array_to_host(d_arr, layer_obj.n_outputs)\n",
    "        c_float_ptr = ctypes.cast(h_ptr, POINTER(c_float))\n",
    "        \n",
    "        host_array = np.ctypeslib.as_array(c_float_ptr, shape=(layer_obj.n_outputs,)).copy()\n",
    "        \n",
    "        return host_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e3e22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_nn = CudaNN(\"./main/libnn.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e32c79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Add time: 0.0722s\n"
     ]
    }
   ],
   "source": [
    "# --- Test 1: Matrix Addition ---\n",
    "rows, cols = 5120, 5120\n",
    "A = np.random.rand(rows, cols).astype(np.float32)\n",
    "B = np.random.rand(rows, cols).astype(np.float32)\n",
    "\n",
    "start = time.time()\n",
    "C = cuda_nn.mat_add(A, B) \n",
    "print(f\"CUDA Add time: {time.time() - start:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b253f984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product: 30720.0\n"
     ]
    }
   ],
   "source": [
    "# --- Test 2: Dot Product ---\n",
    "n = 10240\n",
    "vec_a = np.ones(n, dtype=np.float32)\n",
    "vec_b = np.ones(n, dtype=np.float32) * 3.0\n",
    "\n",
    "dot_res = cuda_nn.dot_product(vec_a, vec_b)\n",
    "print(f\"Dot product: {dot_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56fc8dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Argmax index:  2 (Value: 0.60)\n",
      "Numpy Argmax index: 2 (Value: 0.60)\n",
      "Success: CUDA matches NumPy\n"
     ]
    }
   ],
   "source": [
    "# --- Test 4: Argmax ---\n",
    "# Simulating a neural network's probability output for 5 classes\n",
    "probs = np.array([0.05, 0.15, 0.60, 0.10, 0.10], dtype=np.float32)\n",
    "\n",
    "cuda_idx = cuda_nn.argmax(probs)\n",
    "numpy_idx = np.argmax(probs)\n",
    "\n",
    "print(f\"CUDA Argmax index:  {cuda_idx} (Value: {probs[cuda_idx]:.2f})\")\n",
    "print(f\"Numpy Argmax index: {numpy_idx} (Value: {probs[numpy_idx]:.2f})\")\n",
    "\n",
    "if cuda_idx == numpy_idx:\n",
    "    print(\"Success: CUDA matches NumPy\")\n",
    "else:\n",
    "    print(\"Error: Results do not match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "552c7979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 10,000,000 elements:\n",
      "Method          | Time (s)     | Speedup vs Python \n",
      "--------------------------------------------------\n",
      "Raw Python      | 0.420290     | 1.0x              \n",
      "NumPy           | 0.009469     | 44.39x\n",
      "NumPy (x100)    | 1.021148     | 0.41x\n",
      "CUDA            | 0.017319     | 24.27x\n",
      "CUDA (x100)     | 0.023290     | 18.05x\n",
      "\n",
      "✅ Verification Successful: CUDA matches NumPy\n"
     ]
    }
   ],
   "source": [
    "# --- Test 5: ReLU Activation ---\n",
    "# Array with a mix of positive and negative numbers\n",
    "\n",
    "# 1,000,000 elements\n",
    "N = 10_000_000\n",
    "test_data = np.random.uniform(-10, 10, N).astype(np.float32)\n",
    "\n",
    "# --- 1. Raw Python ---\n",
    "python_list = test_data.tolist()\n",
    "start = time.time()\n",
    "python_res = [x if x > 0 else 0.0 for x in python_list]\n",
    "python_time = time.time() - start\n",
    "\n",
    "# --- 2. NumPy ---\n",
    "start = time.time()\n",
    "numpy_res = np.maximum(0, test_data)\n",
    "numpy_time = time.time() - start\n",
    "\n",
    "# --- 3. NumPy x100 ---\n",
    "start = time.time()\n",
    "for i in range(100): # This is to attempt to combat the overhead of CUDA when benchmarking\n",
    "    numpy_res = np.maximum(0, test_data)\n",
    "numpy_time_100 = time.time() - start\n",
    "\n",
    "# --- 4. CUDA Kernel ---\n",
    "cuda_data = test_data.copy()\n",
    "start = time.time()\n",
    "cuda_nn.relu(cuda_data)\n",
    "cuda_time = time.time() - start\n",
    "\n",
    "# --- 5. CUDA Kernel x100 ---\n",
    "cuda_data_100 = test_data.copy()\n",
    "start = time.time()\n",
    "cuda_nn.relu_100(cuda_data_100)\n",
    "cuda_time_100 = time.time() - start\n",
    "\n",
    "# --- Results ---\n",
    "print(f\"Results for {N:,} elements:\")\n",
    "print(f\"{'Method':<15} | {'Time (s)':<12} | {'Speedup vs Python':<18}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Raw Python':<15} | {python_time:<12.6f} | {'1.0x':<18}\")\n",
    "print(f\"{'NumPy':<15} | {numpy_time:<12.6f} | {python_time/numpy_time:.2f}x\")\n",
    "print(f\"{'NumPy (x100)':<15} | {numpy_time_100:<12.6f} | {python_time/numpy_time_100:.2f}x\")\n",
    "print(f\"{'CUDA':<15} | {cuda_time:<12.6f} | {python_time/cuda_time:.2f}x\")\n",
    "print(f\"{'CUDA (x100)':<15} | {cuda_time_100:<12.6f} | {python_time/cuda_time_100:.2f}x\")\n",
    "\n",
    "# Verification\n",
    "if np.allclose(cuda_data, numpy_res):\n",
    "    print(\"\\n✅ Verification Successful: CUDA matches NumPy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f52eb225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.1  7.2 11.3]\n"
     ]
    }
   ],
   "source": [
    "# Dense layer OOP\n",
    "input_vec = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n",
    "weights = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4], \n",
    "    [0.5, 0.6, 0.7, 0.8], \n",
    "    [0.9, 1.0, 1.1, 1.2]\n",
    "], dtype=np.float32)\n",
    "bias = np.array([0.1, 0.2, 0.3], dtype=np.float32)\n",
    "\n",
    "d_inputs = cuda_nn.copy_array_to_device(input_vec)\n",
    "\n",
    "layer1 = cuda_nn.DenseLayer(cuda_nn.lib,weights,bias)\n",
    "d_output_ptr = layer1.forward(d_inputs)\n",
    "host_output = cuda_nn.copy_device_array_to_host(d_output_ptr,layer1)\n",
    "print(host_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
