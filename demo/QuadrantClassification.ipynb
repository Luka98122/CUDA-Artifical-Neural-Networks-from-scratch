{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84ec3540",
   "metadata": {},
   "source": [
    "# Quadrant Classification (Random Search Training)\n",
    "\n",
    "This notebook:\n",
    "- Creates a 2D dataset labeled by quadrant (4 classes)\n",
    "- Visualizes the dataset\n",
    "- Defines a tiny neural net (2 → 8 → 4) with ReLU + Softmax\n",
    "- Trains using a simple random-search / hill-climb on weights and biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d658a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "027f3e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset(points, classes=4):\n",
    "    X = np.random.uniform(-1, 1, size=(points * classes, 2))\n",
    "    y = np.zeros(points * classes, dtype='uint8')\n",
    "\n",
    "    for i, (x, y_val) in enumerate(X):\n",
    "        if x >= 0 and y_val >= 0:\n",
    "            y[i] = 0  # Quad 1\n",
    "        elif x < 0 and y_val >= 0:\n",
    "            y[i] = 1  # Quad 2\n",
    "        elif x < 0 and y_val < 0:\n",
    "            y[i] = 2  # Quad 3\n",
    "        else:\n",
    "            y[i] = 3  # Quad 4\n",
    "\n",
    "    return X, y\n",
    "X,y = createDataset(1000,4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb88818",
   "metadata": {},
   "source": [
    "## Model components\n",
    "\n",
    "A minimal dense layer, ReLU, Softmax, and Categorical Cross-Entropy loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8dc9f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron_Layer():\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "class Activation_ReLU():\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "class Activation_Softmax():\n",
    "    def forward(self, inputs):\n",
    "        exps = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        return np.mean(sample_losses)\n",
    "\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        return -np.log(correct_confidences)\n",
    "\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    predictions = np.argmax(y_pred, axis=1)\n",
    "    return np.mean(predictions == y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79863869",
   "metadata": {},
   "source": [
    "## Initialize network\n",
    "\n",
    "Network: 2 → 8 → 4 (ReLU then Softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "238fed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial random weights\n",
    "w1 = 0.1 * np.random.randn(2, 8)\n",
    "w2 = 0.1 * np.random.randn(8, 4)\n",
    "\n",
    "# Best weights so far (start with initial)\n",
    "bw1 = w1.copy()\n",
    "bw2 = w2.copy()\n",
    "\n",
    "bloss = 9_999_999\n",
    "\n",
    "# Build network\n",
    "layer1 = Neuron_Layer(2, 8)\n",
    "layer1.weights = w1.copy()\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "layer2 = Neuron_Layer(8, 4)\n",
    "layer2.weights = w2.copy()\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Best biases so far\n",
    "bb1 = layer1.biases.copy()\n",
    "bb2 = layer2.biases.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cc3230",
   "metadata": {},
   "source": [
    "## Training loop (random search / hill climbing)\n",
    "\n",
    "At each iteration:\n",
    "- Propose slightly perturbed weights/biases\n",
    "- Keep the change if loss improves, otherwise revert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c267d4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter      0 | loss: 1.3805 | acc: 0.4915\n",
      "Iter   5000 | loss: 1.2215 | acc: 0.7348\n",
      "Iter  10000 | loss: 0.9698 | acc: 0.8073\n",
      "Iter  15000 | loss: 0.6862 | acc: 0.9030\n",
      "Iter  20000 | loss: 0.4624 | acc: 0.9563\n",
      "Iter  25000 | loss: 0.3079 | acc: 0.9690\n",
      "Iter  30000 | loss: 0.2120 | acc: 0.9822\n",
      "Iter  35000 | loss: 0.1544 | acc: 0.9885\n",
      "Iter  40000 | loss: 0.1161 | acc: 0.9922\n",
      "Iter  45000 | loss: 0.0916 | acc: 0.9958\n",
      "Iter  50000 | loss: 0.0741 | acc: 0.9958\n"
     ]
    }
   ],
   "source": [
    "loss_fn = Loss_CategoricalCrossentropy()\n",
    "\n",
    "for i in range(50001):\n",
    "    # Propose new parameters around best known\n",
    "    nw1 = bw1 + 0.0005 * np.random.randn(2, 8)\n",
    "    nw2 = bw2 + 0.0005 * np.random.randn(8, 4)\n",
    "\n",
    "    nb1 = bb1 + 0.0005 * np.random.randn(1, 8)\n",
    "    nb2 = bb2 + 0.0005 * np.random.randn(1, 4)\n",
    "\n",
    "    # Assign proposal\n",
    "    layer1.weights = nw1\n",
    "    layer2.weights = nw2\n",
    "    layer1.biases = nb1\n",
    "    layer2.biases = nb2\n",
    "\n",
    "    # Forward pass\n",
    "    layer1.forward(X)\n",
    "    activation1.forward(layer1.output)\n",
    "    layer2.forward(activation1.output)\n",
    "    activation2.forward(layer2.output)\n",
    "\n",
    "    # Evaluate\n",
    "    los = loss_fn.calculate(activation2.output, y)\n",
    "    acc = accuracy(activation2.output, y)\n",
    "\n",
    "    if i % 5000 == 0:\n",
    "        print(f\"Iter {i:6d} | loss: {los:.4f} | acc: {acc:.4f}\")\n",
    "\n",
    "    # Accept/reject\n",
    "    if los < bloss:\n",
    "        bw1, bw2 = nw1, nw2\n",
    "        bb1, bb2 = nb1, nb2\n",
    "        bloss = los\n",
    "    else:\n",
    "        # revert\n",
    "        layer1.weights = bw1\n",
    "        layer2.weights = bw2\n",
    "        layer1.biases = bb1\n",
    "        layer2.biases = bb2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afbdc4f",
   "metadata": {},
   "source": [
    "## Final evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8caa6cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.07409305430574245\n",
      "Final acc: 0.99575\n"
     ]
    }
   ],
   "source": [
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "layer2.forward(activation1.output)\n",
    "activation2.forward(layer2.output)\n",
    "\n",
    "final_loss = loss_fn.calculate(activation2.output, y)\n",
    "final_acc = accuracy(activation2.output, y)\n",
    "\n",
    "print(\"Final loss:\", final_loss)\n",
    "print(\"Final acc:\", final_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
